{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import SVD\n",
    "from surprise import Dataset\n",
    "from surprise.model_selection import cross_validate\n",
    "from pandas import Series, DataFrame\n",
    "import pandas as pd\n",
    "import re\n",
    "%pylab inline\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "\n",
    "from surprise import SVD\n",
    "from surprise import Dataset\n",
    "\n",
    "from surprise import NormalPredictor\n",
    "\n",
    "from surprise import Reader\n",
    "from surprise import NMF\n",
    "from surprise import SVDpp\n",
    "from surprise import accuracy\n",
    "from surprise.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users=pd.read_csv('animelists_reduced.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df =users[['username','anime_id','my_score']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = Reader(rating_scale=(1, df.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Dataset.load_from_df(df[['username','anime_id','my_score']], reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset, testset = train_test_split(data, test_size=.20)\n",
    "\n",
    "\n",
    "algo = SVD()\n",
    "\n",
    "# Train the algorithm on the trainset, and predict ratings for the testset\n",
    "algo.fit(trainset)\n",
    "predictions = algo.test(testset)\n",
    "\n",
    "# Then compute RMSE\n",
    "accuracy.rmse(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_n(predictions, n=10):\n",
    "\n",
    "\n",
    "    # First map the predictions to each user.\n",
    "    top_n = defaultdict(list)\n",
    "    for uid, iid, true_r, est, _ in predictions:\n",
    "        top_n[uid].append((iid, est, true_r))\n",
    "    #uid:[(iid,est),(iid,est)]\n",
    "    # Then sort the predictions for each user and retrieve the k highest ones.\n",
    "    for uid, user_ratings in top_n.items():\n",
    "        user_ratings.sort(key=lambda x: x[1], reverse=True)\n",
    "        top_n[uid] = user_ratings#[:n]\n",
    "\n",
    "    return top_n\n",
    "top_n = get_top_n(predictions, n=10)\n",
    "#print(top_n)\n",
    "users_est = defaultdict(list)\n",
    "users_true=defaultdict(list)\n",
    "for uid, user_ratings in top_n.items():\n",
    "    users_est[uid].append([est for (_, est,_) in user_ratings])\n",
    "    users_true[uid].append([true_r for (_,_,true_r) in user_ratings])\n",
    "#print (users_true)\n",
    "#print (users_est)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ndcg(y_true, y_pred, k=None, powered=False):\n",
    "    def dcg(scores, k=None, powered=False):\n",
    "        if k is None:\n",
    "            k = scores.shape[0]\n",
    "        if not powered:\n",
    "            ret = scores[0]\n",
    "            for i in range(1, k):\n",
    "                ret += scores[i] / np.log2(i + 1)\n",
    "            return ret\n",
    "        else:\n",
    "            ret = 0\n",
    "            for i in range(k):\n",
    "                ret += (2 ** scores[i] - 1) / np.log2(i + 2)\n",
    "            return ret\n",
    "    \n",
    "    ideal_sorted_scores = np.sort(y_true)[::-1]\n",
    "    ideal_dcg_score = dcg(ideal_sorted_scores, k=k, powered=powered)\n",
    "    \n",
    "    pred_sorted_ind = np.argsort(y_pred)[::-1]\n",
    "    pred_sorted_scores = y_true[pred_sorted_ind]\n",
    "    dcg_score = dcg(pred_sorted_scores, k=k, powered=powered)\n",
    "    \n",
    "    return dcg_score / ideal_dcg_score\n",
    "\n",
    "def ndcg1(y_true, y_pred, k=None):\n",
    "    return ndcg(y_true, y_pred, k=k, powered=False)\n",
    "\n",
    "def ndcg2(y_true, y_pred, k=None):\n",
    "    return ndcg(y_true, y_pred, k=k, powered=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndcg_list=[]\n",
    "for uid in top_n:\n",
    "    \n",
    "    for i in users_true[uid]:\n",
    "        y_true=np.asarray(i)#.reshape(-1,1)\n",
    "    for i in users_est[uid]:\n",
    "        y_pred=np.asarray(i)#.reshape(-1,1)\n",
    "    \n",
    "        ndcg_list.append(ndcg1(y_true, y_pred, k=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndcg_list = [i for i in ndcg_list if str(i) != 'nan']\n",
    "mean(ndcg_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
